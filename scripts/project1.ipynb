{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and testing data into feature matrix, class labels, and event ids:\n",
    "### Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from given_proj1_helpers import *\n",
    "from utilities import *\n",
    "\n",
    "from clean_data import *\n",
    "from features_processing import *\n",
    "\n",
    "from given_costs import *\n",
    "from implementations import *\n",
    "from validation import *\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Train data\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "# Replace -999 values by NaN.\n",
    "tX_NaN = np.copy(tX)\n",
    "tX_NaN[tX == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test data\n",
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# Replace -999 values by NaN.\n",
    "tX_test_NaN = np.copy(tX_test)\n",
    "tX_test_NaN[tX_test == -999] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_all_histograms(y, tX, clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_all_histograms(y, tX, clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing\n",
    "\n",
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_standardized, _, _ = standardize(tX)\n",
    "tX_standardized_NaN, _, _ = standardize_NaN(tX_NaN)\n",
    "tX_standardized_NaN_poly, _, _ = standardize_NaN_poly(tX_NaN)\n",
    "\n",
    "tX_test_standardized, _, _ = standardize(tX_test)\n",
    "tX_test_standardized_NaN, _, _ = standardize_NaN(tX_test_NaN)\n",
    "tX_test_standardized_NaN_poly, _, _ = standardize_NaN_poly(tX_test_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of Features Expension (adding feature dimension and polynomial expension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_expend_data():\n",
    "\n",
    "    arr = np.array([[1, 2, 3, 4, 5], [6, -7, 8, 9, 10], [11, 12, 13, 14, 14]])\n",
    "    print(arr)\n",
    "    print(\"\\n\")\n",
    "    print(add_inverse_log_features(arr, [0,1, 2, 3, 4]))\n",
    "    print(build_poly_mixed_term(arr, [0, 1], 2))\n",
    "\n",
    "#test_expend_data()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_remove_features():\n",
    "    a = np.array([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5],[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "    a = remove_features(a, [1, 3])\n",
    "    print(a)\n",
    "    \n",
    "#test_remove_features()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_keep_features():\n",
    "    a = np.array([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5],[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "    a = keep_features(a, [1, 3])\n",
    "    print (a)\n",
    "    \n",
    "#test_keep_features()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2 - Tests of Implement ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_least_squares(y, tx):\n",
    "   \n",
    "    w, L= least_squares(y, tx)\n",
    "    \n",
    "    print(\"ls = {}\".format(L))\n",
    "    \n",
    "#test_least_squares(y, tX)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_least_squares_GD():\n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 2005\n",
    "    gamma = 0.1\n",
    "\n",
    "    # Initialization\n",
    "    w_ls, _ = least_squares(y, tX_standardized)\n",
    "    w_initial = w_ls #+ (np.mean(w_ls) + 0.001*np.random.randn())\n",
    "\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_gd, L_gd = least_squares_GD(y, tX_standardized, w_initial, max_iters, gamma)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_gd, w0=w_gd[0], w1=w_gd[1]))\n",
    "    \n",
    "#test_least_squares_GD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_least_squares_SGD():\n",
    "    max_iters = 501\n",
    "    gamma = 0.15\n",
    "\n",
    "    # Initialization\n",
    "    w_ls, _ = least_squares(y, tX_standardized)\n",
    "    w_initial = w_ls #+ (np.mean(w_ls) + 0.001*np.random.randn())\n",
    "\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w, ls = least_squares_SGD(y, tX_standardized,w_initial, max_iters, gamma )\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # This can be used if ws and ls are arrays of values.\n",
    "    #min_loss = ls[np.argmin(ls)]\n",
    "    #best_w = ws[np.argmin(ls) + 1]\n",
    "    print(\"MSE:\", ls)\n",
    "    \n",
    "    compute_performance(y, tX_standardized, w)\n",
    "      \n",
    "    #print(\"Socastic Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    #print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_sgd, w0=w_sgd[0], w1=w_sgd[1]))\n",
    "    \n",
    "#test_least_squares_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression():\n",
    "    \n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 20000\n",
    "    gamma = 0.00000001\n",
    "    \n",
    "    # Start logistic regression with gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_lr, L_lr = logistic_regression(y, tX_standardized[:,1:], gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic regression with Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_lr, w0=w_lr[0], w1=w_lr[1]))\n",
    "    print(compute_loss(y, tX_standardized[:,1:], np.ndarray.flatten(w_lr)))\n",
    "    \n",
    "#test_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_reg_logistic_regression():\n",
    "    \n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 20000\n",
    "    gamma = 0.0000001\n",
    "    lambda_ = 0.01\n",
    "    # Start logistic regression with gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_lr, L_lr = reg_logistic_regression(y, tX_standardized[:,1:], lambda_, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic regression with Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_lr, w0=w_lr[0], w1=w_lr[1]))\n",
    "    print(compute_loss(y, tX_standardized[:,1:], np.ndarray.flatten(w_lr)))\n",
    "    \n",
    "#test_reg_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression_newton():\n",
    "    \n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 20000\n",
    "    gamma = 0.001\n",
    "    \n",
    "    # Start logistic regression with gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_lr, L_lr = logistic_regression_newton(y, tX_standardized[:,1:], gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic regression with Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_lr, w0=w_lr[0], w1=w_lr[1]))\n",
    "    print(compute_loss(y, tX_standardized[:,1:], np.ndarray.flatten(w_lr)))\n",
    "\n",
    "###????Memory error    \n",
    "#test_logistic_regression_newton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_reg_logistic_regression_newton():\n",
    "    \n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 20000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 0.01\n",
    "    # Start logistic regression with gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_lr, L_lr = reg_logistic_regression_newton(y, tX_standardized[:,1:], lambda_, gamma, max_iters)\n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Logistic regression with Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    print(\"best_loss={l}, best_w0={w0}, best_w1={w1}\".format(l=L_lr, w0=w_lr[0], w1=w_lr[1]))\n",
    "    print(compute_loss(y, tX_standardized[:,1:], np.ndarray.flatten(w_lr)))\n",
    "\n",
    "###????Memory error        \n",
    "#test_reg_logistic_regression_newton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_best_parameters(y, tx):\n",
    "    tx = replaceNaNbyMostFreq(tx)\n",
    "    y, tx = select_random(y, tx, 50000)\n",
    "    x_train, y_train, x_test, y_test = split_data(y, tx, 0.7)\n",
    "\n",
    "    x_train,_,_ = standardizeSameDimension(x_train)\n",
    "    x_test,_,_ = standardizeSameDimension(x_test)\n",
    "\n",
    "    #EV, k = pca(x_train, 0.9)\n",
    "    #x_train = np.dot(x_train, EV)\n",
    "    #x_test = np.dot(x_test, EV)\n",
    "    \n",
    "    d, w, l, s = find_best_degree_and_lambda(y_train, y_test, x_train, x_test, 8)\n",
    "    \n",
    "    print('The best score is for degree = ', str(d), ' and lambda = ', str(l))\n",
    "    print('The resulting score is', s)\n",
    "    \n",
    "    return d, l\n",
    "    \n",
    "#compute_best_parameters(y, tX_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ilog, log, sin, poly and no composition => d=3, l=0.0162975083462\n",
    "#ilog, new_log, sin, poly and no composition => d=3, l=31.2571584969 => loss = 0.819333\n",
    "#ilog, new_log, sin, poly and composition => d=2, l=2.71858824273 => loss = 0.8186\n",
    "#ilog, sin, poly and no composition => d=5, l=0.47508101621 => loss=0.817533333333\n",
    "#ilog, sin, poly and composition => d=3, l=31.2571584969 => loss=0.816133333333\n",
    "#new_log, sin, poly and composition => d=3, l=44.3062145758 => loss = 0.820333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_poly_cross_validation(y, tx, k_fold, degree, debug):\n",
    "    #degrees = np.arange(degree)\n",
    "    degrees = np.array([3])\n",
    "    lambdas = np.array([0.01])\n",
    "    \n",
    "    #num = 1000\n",
    "    #mse_tr, mse_te, scores = cross_validation_poly(y[0:num], tx[0:num, 0:30], k_fold, degrees, lambdas, debug)\n",
    "    mse_tr, mse_te, scores = cross_validation_poly(y, tx, k_fold, degrees, lambdas, debug)\n",
    "    \n",
    "    cross_validation_visualization_degrees(degrees, mse_tr, mse_te)\n",
    "    #cross_validation_visualization_lambdas(lambdas, mse_tr, mse_te)\n",
    "    \n",
    "#test_poly_cross_validation(y, tX_standardized_NaN_poly, 4, 6, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:\n",
    "## Prepare test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  0.263641059244\n"
     ]
    }
   ],
   "source": [
    "def compute_prediction(y, tx, tx_test):\n",
    "    tx = replaceNaNbyMostFreq(tx)\n",
    "    tx_test = replaceNaNbyMostFreq(tx_test)\n",
    "    tx,_,_ = standardizeSameDimension(tx)\n",
    "    tx_test,_,_ = standardizeSameDimension(tx_test)\n",
    "    \n",
    "    tx = add_new_log_features(tx, np.arange(tx.shape[1]))\n",
    "    tx_test = add_new_log_features(tx_test, np.arange(tx_test.shape[1]))\n",
    "    \n",
    "    tx = add_sin_features(tx, np.arange(tx.shape[1]))\n",
    "    tx_test = add_sin_features(tx_test, np.arange(tx_test.shape[1]))\n",
    "    \n",
    "    tx = add_root_features(tx, np.arange(tx.shape[1]))\n",
    "    tx_test = add_root_features(tx_test, np.arange(tx_test.shape[1]))\n",
    "    \n",
    "    tx,_,_ = standardizeSameDimension(tx)\n",
    "    tx_test,_,_ = standardizeSameDimension(tx_test)\n",
    "    \n",
    "    tx = build_d_poly(tx, 3)\n",
    "    tx_test = build_d_poly(tx_test, 3)\n",
    "    \n",
    "    w, loss = ridge_regression(y, tx, 107.226722201)\n",
    "    \n",
    "    print('Loss is ', loss)\n",
    "    \n",
    "    y_pred = predict_labels(w, tx_test)\n",
    "    \n",
    "    return y_pred, w, loss\n",
    "\n",
    "y_pred, w, loss = compute_prediction(y, tX_NaN, tX_test_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'outputs/output.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
